{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "generative-adversarial-network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V8qfRfWjTSx",
        "colab_type": "code",
        "outputId": "57473be1-232f-4745-f898-db211beb1894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:5 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [852 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [59.3 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [930 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8,815 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,818 kB]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [92.5 kB]\n",
            "Hit:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [73.6 kB]\n",
            "Hit:18 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [20.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,385 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,228 kB]\n",
            "Get:24 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [877 kB]\n",
            "Fetched 7,616 kB in 3s (3,012 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 60 not upgraded.\n",
            "Need to get 77.3 MB of archives.\n",
            "After this operation, 264 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 81.0.4044.138-0ubuntu0.18.04.1 [1,095 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 81.0.4044.138-0ubuntu0.18.04.1 [68.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 81.0.4044.138-0ubuntu0.18.04.1 [3,231 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 81.0.4044.138-0ubuntu0.18.04.1 [4,079 kB]\n",
            "Fetched 77.3 MB in 3s (24.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_81.0.4044.138-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Yj68ahe67R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import keras\n",
        "import warnings\n",
        "import random\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation\n",
        "from keras.optimizers import RMSprop, Adam, Adadelta, SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "from keras import backend as K\n",
        "\n",
        "# Display preference\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijjx2FEqe67i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scroll(driver, timeout):\n",
        "    \n",
        "    # Get scroll height\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    while True:\n",
        "    \n",
        "        # Scroll down to bottom\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "        # Wait\n",
        "        time.sleep(timeout)\n",
        "\n",
        "        # Calculate new scroll height and compare with last scroll height\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        \n",
        "        if new_height == last_height:\n",
        "            \n",
        "            break\n",
        "        \n",
        "        last_height = new_height   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haGA3aSLe67v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "def all_links(url, action, gender=0):\n",
        "    \n",
        "    # Setup driver\n",
        "    driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "\n",
        "    # Wait before throwing an exception\n",
        "    driver.implicitly_wait(20)\n",
        "\n",
        "    # Open the page\n",
        "    driver.get(url)\n",
        "    \n",
        "    # Start scrolling\n",
        "    scroll(driver, 10)\n",
        "    \n",
        "    # Beautiful Soup parses page_source\n",
        "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "    \n",
        "    # Close the driver\n",
        "    driver.close()\n",
        "\n",
        "    if action == 1:\n",
        "    \n",
        "        # Empty dictionary to store m from page source\n",
        "        get_m = {}\n",
        "    \n",
        "        # Counter for get_m dictionary\n",
        "        get_m_counter = 0\n",
        "\n",
        "        # Looping through all the a elements in the page source\n",
        "        for link in soup.find_all('a', {'class': 'iusc'}):\n",
        "    \n",
        "            # Add values to dictionary\n",
        "            get_m[get_m_counter] = link.get('m')\n",
        "        \n",
        "            # Increment counter\n",
        "            get_m_counter += 1\n",
        "        \n",
        "        # Convert from string to dictionary\n",
        "        for key, value in get_m.items():\n",
        "            get_m[key] = json.loads(value)\n",
        "        \n",
        "        # Empty list to store image links\n",
        "        links = []\n",
        "    \n",
        "        # Loop through get_m dictionary and add image links to links list\n",
        "        for value in get_m.values():\n",
        "            links.append([value['murl'], gender])\n",
        "        \n",
        "        return links\n",
        "    else:\n",
        "        return soup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBet4uG4e673",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of names to search\n",
        "names = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJh0voVe67_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Popular names url\n",
        "name_url = 'https://www.ssa.gov/oact/babynames/decades/century.html'\n",
        "\n",
        "# Beautiful Soup object\n",
        "soup_name = all_links(name_url, 0)\n",
        "\n",
        "# Get td tag\n",
        "td_tag = soup_name.select('.t-stripe > tbody > tr > td')\n",
        "\n",
        "# Add name to search names\n",
        "names += [name.get_text().lower() for name in td_tag if name.get_text().isalpha()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdvHLEoe68G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List for male names\n",
        "male_names = []\n",
        "\n",
        "# List for female names\n",
        "female_names = []\n",
        "\n",
        "for index, name in enumerate(names, 0):\n",
        "    if index % 2 == 0:\n",
        "        male_names.append([name, 0])\n",
        "    else:\n",
        "        female_names.append([name, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUVnZZNqe68N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get image links for each name\n",
        "def get_links(names):\n",
        "    \n",
        "    image_links_name = []\n",
        "    pre_image_links_count = 0\n",
        "    post_image_links_count = 0\n",
        "    \n",
        "    for index in range(len(names)):\n",
        "        \n",
        "        # Update url\n",
        "        url = 'https://www.bing.com/images/search?q=' + names[index][0] + '&qs=n&form=QBIR&qft=%20filterui%3Alicense-L2_L3_L4_L5_L6_L7%20filterui%3Aface-face&sp=-1&pq=' + names[index][0] + '&sc=8-9&sk=&cvid=8C2D8EC9A35A4948980A61669549C180'    \n",
        "    \n",
        "        # Number of image links before\n",
        "        pre_image_links_count = len(image_links_name)\n",
        "    \n",
        "        # Add image links\n",
        "        image_links_name += all_links(url, 1, names[index][1])\n",
        "        \n",
        "        # Number of image links after\n",
        "        post_image_links_count = len(image_links_name)\n",
        "        \n",
        "        # Number of image links associated with name\n",
        "        print('{} resulted in {} image links'.format(names[index][0], (post_image_links_count - pre_image_links_count)))\n",
        "        \n",
        "    return image_links_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPEVB4dze68S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List for image links\n",
        "image_links = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GWxb_pxKe68X",
        "colab_type": "code",
        "outputId": "9617cda9-6f9d-4e59-ef2d-52aaa9827102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Add male name image links\n",
        "image_links += get_links(male_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "james resulted in 175 image links\n",
            "john resulted in 175 image links\n",
            "robert resulted in 175 image links\n",
            "michael resulted in 175 image links\n",
            "william resulted in 175 image links\n",
            "david resulted in 175 image links\n",
            "richard resulted in 210 image links\n",
            "joseph resulted in 175 image links\n",
            "thomas resulted in 175 image links\n",
            "charles resulted in 175 image links\n",
            "christopher resulted in 175 image links\n",
            "daniel resulted in 175 image links\n",
            "matthew resulted in 175 image links\n",
            "anthony resulted in 175 image links\n",
            "donald resulted in 175 image links\n",
            "mark resulted in 210 image links\n",
            "paul resulted in 175 image links\n",
            "steven resulted in 175 image links\n",
            "andrew resulted in 175 image links\n",
            "kenneth resulted in 175 image links\n",
            "joshua resulted in 175 image links\n",
            "george resulted in 175 image links\n",
            "kevin resulted in 175 image links\n",
            "brian resulted in 175 image links\n",
            "edward resulted in 175 image links\n",
            "ronald resulted in 175 image links\n",
            "timothy resulted in 175 image links\n",
            "jason resulted in 175 image links\n",
            "jeffrey resulted in 175 image links\n",
            "ryan resulted in 175 image links\n",
            "jacob resulted in 175 image links\n",
            "gary resulted in 175 image links\n",
            "nicholas resulted in 175 image links\n",
            "eric resulted in 175 image links\n",
            "stephen resulted in 210 image links\n",
            "jonathan resulted in 175 image links\n",
            "larry resulted in 175 image links\n",
            "justin resulted in 175 image links\n",
            "scott resulted in 175 image links\n",
            "brandon resulted in 175 image links\n",
            "frank resulted in 175 image links\n",
            "benjamin resulted in 175 image links\n",
            "gregory resulted in 175 image links\n",
            "samuel resulted in 175 image links\n",
            "raymond resulted in 175 image links\n",
            "patrick resulted in 175 image links\n",
            "alexander resulted in 175 image links\n",
            "jack resulted in 175 image links\n",
            "dennis resulted in 175 image links\n",
            "jerry resulted in 175 image links\n",
            "tyler resulted in 175 image links\n",
            "aaron resulted in 175 image links\n",
            "jose resulted in 175 image links\n",
            "henry resulted in 175 image links\n",
            "douglas resulted in 175 image links\n",
            "adam resulted in 175 image links\n",
            "peter resulted in 175 image links\n",
            "nathan resulted in 175 image links\n",
            "zachary resulted in 175 image links\n",
            "walter resulted in 175 image links\n",
            "kyle resulted in 175 image links\n",
            "harold resulted in 175 image links\n",
            "carl resulted in 481 image links\n",
            "jeremy resulted in 175 image links\n",
            "keith resulted in 175 image links\n",
            "roger resulted in 175 image links\n",
            "gerald resulted in 175 image links\n",
            "ethan resulted in 175 image links\n",
            "arthur resulted in 175 image links\n",
            "terry resulted in 175 image links\n",
            "christian resulted in 175 image links\n",
            "sean resulted in 175 image links\n",
            "lawrence resulted in 175 image links\n",
            "austin resulted in 175 image links\n",
            "joe resulted in 175 image links\n",
            "noah resulted in 175 image links\n",
            "jesse resulted in 175 image links\n",
            "albert resulted in 175 image links\n",
            "bryan resulted in 175 image links\n",
            "billy resulted in 175 image links\n",
            "bruce resulted in 175 image links\n",
            "willie resulted in 210 image links\n",
            "jordan resulted in 175 image links\n",
            "dylan resulted in 175 image links\n",
            "alan resulted in 175 image links\n",
            "ralph resulted in 175 image links\n",
            "gabriel resulted in 175 image links\n",
            "roy resulted in 175 image links\n",
            "juan resulted in 175 image links\n",
            "wayne resulted in 175 image links\n",
            "eugene resulted in 175 image links\n",
            "logan resulted in 175 image links\n",
            "randy resulted in 210 image links\n",
            "louis resulted in 175 image links\n",
            "russell resulted in 175 image links\n",
            "vincent resulted in 175 image links\n",
            "philip resulted in 175 image links\n",
            "bobby resulted in 175 image links\n",
            "johnny resulted in 175 image links\n",
            "bradley resulted in 175 image links\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AfG_B2DCe68e",
        "colab_type": "code",
        "outputId": "23c26992-95b9-4bb9-d575-cb2bff0adfc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Add female name image links\n",
        "image_links += get_links(female_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mary resulted in 175 image links\n",
            "patricia resulted in 175 image links\n",
            "jennifer resulted in 175 image links\n",
            "linda resulted in 175 image links\n",
            "elizabeth resulted in 175 image links\n",
            "barbara resulted in 175 image links\n",
            "susan resulted in 175 image links\n",
            "jessica resulted in 175 image links\n",
            "sarah resulted in 175 image links\n",
            "karen resulted in 175 image links\n",
            "nancy resulted in 175 image links\n",
            "margaret resulted in 175 image links\n",
            "lisa resulted in 175 image links\n",
            "betty resulted in 175 image links\n",
            "dorothy resulted in 175 image links\n",
            "sandra resulted in 567 image links\n",
            "ashley resulted in 175 image links\n",
            "kimberly resulted in 175 image links\n",
            "donna resulted in 175 image links\n",
            "emily resulted in 175 image links\n",
            "michelle resulted in 175 image links\n",
            "carol resulted in 175 image links\n",
            "amanda resulted in 175 image links\n",
            "melissa resulted in 175 image links\n",
            "deborah resulted in 175 image links\n",
            "stephanie resulted in 175 image links\n",
            "rebecca resulted in 175 image links\n",
            "laura resulted in 175 image links\n",
            "sharon resulted in 175 image links\n",
            "cynthia resulted in 175 image links\n",
            "kathleen resulted in 175 image links\n",
            "helen resulted in 175 image links\n",
            "amy resulted in 175 image links\n",
            "shirley resulted in 175 image links\n",
            "angela resulted in 175 image links\n",
            "anna resulted in 175 image links\n",
            "brenda resulted in 175 image links\n",
            "pamela resulted in 175 image links\n",
            "nicole resulted in 175 image links\n",
            "ruth resulted in 175 image links\n",
            "katherine resulted in 175 image links\n",
            "samantha resulted in 175 image links\n",
            "christine resulted in 175 image links\n",
            "emma resulted in 175 image links\n",
            "catherine resulted in 175 image links\n",
            "debra resulted in 175 image links\n",
            "virginia resulted in 175 image links\n",
            "rachel resulted in 175 image links\n",
            "carolyn resulted in 175 image links\n",
            "janet resulted in 175 image links\n",
            "maria resulted in 175 image links\n",
            "heather resulted in 175 image links\n",
            "diane resulted in 175 image links\n",
            "julie resulted in 175 image links\n",
            "joyce resulted in 175 image links\n",
            "victoria resulted in 175 image links\n",
            "kelly resulted in 6 image links\n",
            "christina resulted in 175 image links\n",
            "joan resulted in 175 image links\n",
            "evelyn resulted in 175 image links\n",
            "lauren resulted in 175 image links\n",
            "judith resulted in 175 image links\n",
            "olivia resulted in 210 image links\n",
            "frances resulted in 175 image links\n",
            "martha resulted in 175 image links\n",
            "cheryl resulted in 175 image links\n",
            "megan resulted in 175 image links\n",
            "andrea resulted in 175 image links\n",
            "hannah resulted in 175 image links\n",
            "jacqueline resulted in 175 image links\n",
            "ann resulted in 175 image links\n",
            "jean resulted in 175 image links\n",
            "alice resulted in 175 image links\n",
            "kathryn resulted in 175 image links\n",
            "gloria resulted in 175 image links\n",
            "teresa resulted in 175 image links\n",
            "doris resulted in 175 image links\n",
            "sara resulted in 175 image links\n",
            "janice resulted in 175 image links\n",
            "julia resulted in 175 image links\n",
            "marie resulted in 175 image links\n",
            "madison resulted in 175 image links\n",
            "grace resulted in 175 image links\n",
            "judy resulted in 175 image links\n",
            "theresa resulted in 175 image links\n",
            "beverly resulted in 175 image links\n",
            "denise resulted in 210 image links\n",
            "marilyn resulted in 175 image links\n",
            "amber resulted in 175 image links\n",
            "danielle resulted in 175 image links\n",
            "abigail resulted in 175 image links\n",
            "brittany resulted in 175 image links\n",
            "rose resulted in 175 image links\n",
            "diana resulted in 175 image links\n",
            "natalie resulted in 175 image links\n",
            "sophia resulted in 175 image links\n",
            "alexis resulted in 175 image links\n",
            "lori resulted in 175 image links\n",
            "kayla resulted in 175 image links\n",
            "jane resulted in 175 image links\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN6QeN25e68l",
        "colab_type": "code",
        "outputId": "5a841531-5dbd-476c-93dd-b3de89f6fcae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('There are {} image links'.format(len(image_links)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 35774 image links\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5yqzsfVIe68r",
        "colab_type": "code",
        "outputId": "62cf7395-45ef-4fa7-8b27-f2e300e0bb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Header to accompany get request\n",
        "headers = {'user-agent': 'scraper - school project (bengriffith@outlook.com)'}\n",
        "\n",
        "# Counter to be used in image file name\n",
        "image_counter = 0\n",
        "\n",
        "# Counter for exceptions occurred\n",
        "exception_counter = 0\n",
        "\n",
        "# Write images to disk\n",
        "for image_link in image_links:\n",
        "    with open('../gdrive/My Drive/colab/gan/images/face_' + str(image_counter) + '_' + str(image_link[1]) + '.png', 'wb') as f:\n",
        "        try:\n",
        "            response = requests.get(image_link[0], headers=headers)\n",
        "            \n",
        "            if response.ok:\n",
        "                f.write(response.content)\n",
        "        except:\n",
        "            exception_counter += 1\n",
        "            continue\n",
        "    image_counter += 1\n",
        "    \n",
        "print('There were {} files removed due to a Request / Response Exception'.format(exception_counter))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 76 files removed due to a Request / Response Exception\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsTcyZFze68x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_directory = '../gdrive/My Drive/colab/gan/images/'\n",
        "images_cropped_directory = '../gdrive/My Drive/colab/gan/images_cropped/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yxfSYhYe683",
        "colab_type": "code",
        "outputId": "47633c52-df30-4e38-b7d8-304cafddfe22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove images that create AttributeError\n",
        "files_removed = 0\n",
        "\n",
        "for filename in sorted(os.listdir(images_directory))[1:]:\n",
        "    \n",
        "    try:\n",
        "        image = cv2.imread(images_directory + filename)\n",
        "        (h, w) = image.shape[:2]\n",
        "    except AttributeError:\n",
        "        files_removed += 1\n",
        "        os.remove(images_directory + filename)\n",
        "        \n",
        "print('There were {} files removed due to AttributeError'.format(files_removed))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 0 files removed due to AttributeError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdjA62hYe689",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prototxt = '../gdrive/My Drive/colab/gan/supporting/deploy.prototxt'\n",
        "model = '../gdrive/My Drive/colab/gan/supporting/res10_300x300_ssd_iter_140000.caffemodel'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e-mDf1Ge69D",
        "colab_type": "code",
        "outputId": "7d9d2e46-d15c-4b31-e24e-6df840e83698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file_counter = 0\n",
        "\n",
        "for filename in sorted(os.listdir(images_directory))[1:]:\n",
        "\n",
        "    # Image path\n",
        "    image_path = images_directory + filename\n",
        "    \n",
        "    # Load model\n",
        "    net = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
        "\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    (h, w) = image.shape[:2]\n",
        "    \n",
        "    # Blob for the image\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    \n",
        "    # Pass blob through network and retrieve detections / predictions\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "    \n",
        "    for i in range(detections.shape[2]):\n",
        "        \n",
        "        # Get confidence associated with the prediction\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "    \n",
        "        if confidence > 0.98:\n",
        "            \n",
        "            # Coordinates for the bounding box\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype('int')\n",
        "        \n",
        "            # Coordinates for face\n",
        "            face = image[startY:endY, startX:endX]\n",
        "        \n",
        "            # Save face image \n",
        "            cv2.imwrite(images_cropped_directory + filename, face)\n",
        "\n",
        "            file_counter += 1\n",
        "        \n",
        "print('There were {} face images added to images_cropped directory'.format(file_counter))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 34802 face images added to images_cropped directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_GUSHtFe69I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List for image data\n",
        "image_data = []\n",
        "\n",
        "# List to store detected face\n",
        "data = []\n",
        "\n",
        "# List to store labels\n",
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQVDgkROe69Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Populate image_data list\n",
        "for filename in sorted(os.listdir(images_cropped_directory)):\n",
        "    image_path = images_cropped_directory + filename\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "    image_data.append([image, int(filename[filename.find('.') - 1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve00OqZJe69Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate image data into data list and labels list\n",
        "for image in image_data:\n",
        "    data.append(image[0])\n",
        "    labels.append(image[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5g8Fje6e69f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Numpy arrays\n",
        "data = np.array(data, dtype='float') / 255.0\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mTez3Rle69j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9rAEKcQ039H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4ca184c4-6ccc-4de2-d427-963274f4b2f8"
      },
      "source": [
        "print('There are {} records in the train dataset'.format(X_train.shape[0]))\n",
        "print('There are {} records in the test dataset'.format(X_test.shape[0]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 27709 records in the train dataset\n",
            "There are 6928 records in the test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVOCPv-be69n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(height, width, depth, num_classes):\n",
        "    model = Sequential()\n",
        "    input_shape = (height, width, depth)\n",
        "    channel_dimension = -1\n",
        "    \n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='sigmoid'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs21-WXye69s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 25\n",
        "batch_size = 32\n",
        "image_dimensions = (64, 64, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brRieXBpe69v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_model = model(image_dimensions[0], image_dimensions[1], image_dimensions[2], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lblXrjJCe69z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eX7NnSB9e693",
        "colab_type": "code",
        "outputId": "402536f1-641b-478c-a972-8a199c094ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "H = gender_model.fit(X_train, y_train, batch_size=batch_size, \n",
        "                     validation_data=(X_test, y_test), \n",
        "                     epochs=epochs, \n",
        "                     verbose=1)\n",
        "score = gender_model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 27709 samples, validate on 6928 samples\n",
            "Epoch 1/25\n",
            "27709/27709 [==============================] - 276s 10ms/step - loss: 0.3879 - accuracy: 0.8341 - val_loss: 0.2907 - val_accuracy: 0.8887\n",
            "Epoch 2/25\n",
            "27709/27709 [==============================] - 276s 10ms/step - loss: 0.2942 - accuracy: 0.8866 - val_loss: 0.2640 - val_accuracy: 0.9047\n",
            "Epoch 3/25\n",
            "27709/27709 [==============================] - 277s 10ms/step - loss: 0.2489 - accuracy: 0.9090 - val_loss: 0.2440 - val_accuracy: 0.9187\n",
            "Epoch 4/25\n",
            "27709/27709 [==============================] - 277s 10ms/step - loss: 0.2079 - accuracy: 0.9266 - val_loss: 0.2371 - val_accuracy: 0.9209\n",
            "Epoch 5/25\n",
            "27709/27709 [==============================] - 275s 10ms/step - loss: 0.1749 - accuracy: 0.9381 - val_loss: 0.2452 - val_accuracy: 0.9212\n",
            "Epoch 6/25\n",
            "27709/27709 [==============================] - 276s 10ms/step - loss: 0.1435 - accuracy: 0.9514 - val_loss: 0.2443 - val_accuracy: 0.9234\n",
            "Epoch 7/25\n",
            "27709/27709 [==============================] - 276s 10ms/step - loss: 0.1187 - accuracy: 0.9592 - val_loss: 0.2632 - val_accuracy: 0.9238\n",
            "Epoch 8/25\n",
            "27709/27709 [==============================] - 276s 10ms/step - loss: 0.1017 - accuracy: 0.9635 - val_loss: 0.2847 - val_accuracy: 0.9265\n",
            "Epoch 9/25\n",
            "27709/27709 [==============================] - 274s 10ms/step - loss: 0.0896 - accuracy: 0.9692 - val_loss: 0.2946 - val_accuracy: 0.9307\n",
            "Epoch 10/25\n",
            "27709/27709 [==============================] - 272s 10ms/step - loss: 0.0768 - accuracy: 0.9733 - val_loss: 0.3082 - val_accuracy: 0.9267\n",
            "Epoch 11/25\n",
            "27709/27709 [==============================] - 273s 10ms/step - loss: 0.0713 - accuracy: 0.9744 - val_loss: 0.3234 - val_accuracy: 0.9275\n",
            "Epoch 12/25\n",
            "27709/27709 [==============================] - 270s 10ms/step - loss: 0.0613 - accuracy: 0.9794 - val_loss: 0.3570 - val_accuracy: 0.9262\n",
            "Epoch 13/25\n",
            "27709/27709 [==============================] - 271s 10ms/step - loss: 0.0578 - accuracy: 0.9799 - val_loss: 0.3569 - val_accuracy: 0.9258\n",
            "Epoch 14/25\n",
            "27709/27709 [==============================] - 270s 10ms/step - loss: 0.0546 - accuracy: 0.9815 - val_loss: 0.3277 - val_accuracy: 0.9304\n",
            "Epoch 15/25\n",
            "27709/27709 [==============================] - 272s 10ms/step - loss: 0.0511 - accuracy: 0.9820 - val_loss: 0.3709 - val_accuracy: 0.9209\n",
            "Epoch 16/25\n",
            "27709/27709 [==============================] - 270s 10ms/step - loss: 0.0489 - accuracy: 0.9827 - val_loss: 0.4004 - val_accuracy: 0.9257\n",
            "Epoch 17/25\n",
            "27709/27709 [==============================] - 272s 10ms/step - loss: 0.0487 - accuracy: 0.9833 - val_loss: 0.4550 - val_accuracy: 0.9287\n",
            "Epoch 18/25\n",
            "27709/27709 [==============================] - 273s 10ms/step - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.4038 - val_accuracy: 0.9310\n",
            "Epoch 19/25\n",
            "27709/27709 [==============================] - 273s 10ms/step - loss: 0.0426 - accuracy: 0.9855 - val_loss: 0.3955 - val_accuracy: 0.9323\n",
            "Epoch 20/25\n",
            "27709/27709 [==============================] - 272s 10ms/step - loss: 0.0415 - accuracy: 0.9843 - val_loss: 0.4235 - val_accuracy: 0.9291\n",
            "Epoch 21/25\n",
            "27709/27709 [==============================] - 274s 10ms/step - loss: 0.0421 - accuracy: 0.9852 - val_loss: 0.4552 - val_accuracy: 0.9301\n",
            "Epoch 22/25\n",
            "27709/27709 [==============================] - 273s 10ms/step - loss: 0.0374 - accuracy: 0.9860 - val_loss: 0.4030 - val_accuracy: 0.9316\n",
            "Epoch 23/25\n",
            "27709/27709 [==============================] - 274s 10ms/step - loss: 0.0337 - accuracy: 0.9876 - val_loss: 0.4897 - val_accuracy: 0.9311\n",
            "Epoch 24/25\n",
            "27709/27709 [==============================] - 275s 10ms/step - loss: 0.0401 - accuracy: 0.9860 - val_loss: 0.4989 - val_accuracy: 0.9264\n",
            "Epoch 25/25\n",
            "27709/27709 [==============================] - 278s 10ms/step - loss: 0.0396 - accuracy: 0.9861 - val_loss: 0.4521 - val_accuracy: 0.9251\n",
            "Test loss: 0.4521252429435421\n",
            "Test accuracy: 0.9250866174697876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Xs6U-pe697",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train neural network to recognize whether male or female\n",
        "\n",
        "# If female then choose female GAN\n",
        "\n",
        "# If male then choose male GAN\n",
        "\n",
        "# Will need to create / train female and male GANs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}