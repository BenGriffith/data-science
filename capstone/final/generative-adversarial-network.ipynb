{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import keras\n",
    "import warnings\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "from keras import backend as K\n",
    "\n",
    "# Display preference\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll(driver, timeout):\n",
    "    \n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait\n",
    "        time.sleep(timeout)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            \n",
    "            break\n",
    "        \n",
    "        last_height = new_height   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options\n",
    "options = Options()\n",
    "options.set_preference('permissions.default.image', 2)\n",
    "options.set_preference('dom.ipc.plugins.enabled.libflashplayer.so', False)\n",
    "\n",
    "def all_links(url, action, gender=0):\n",
    "    \n",
    "    # Setup driver\n",
    "    driver = webdriver.Firefox(options=options, executable_path='D:/Users/bengriffith/Desktop/geckodriver.exe')\n",
    "\n",
    "    # Wait before throwing an exception\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    # Open the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Start scrolling\n",
    "    scroll(driver, 10)\n",
    "    \n",
    "    # Beautiful Soup parses page_source\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # Close the driver\n",
    "    driver.close()\n",
    "\n",
    "    if action == 1:\n",
    "    \n",
    "        # Empty dictionary to store m from page source\n",
    "        get_m = {}\n",
    "    \n",
    "        # Counter for get_m dictionary\n",
    "        get_m_counter = 0\n",
    "\n",
    "        # Looping through all the a elements in the page source\n",
    "        for link in soup.find_all('a', {'class': 'iusc'}):\n",
    "    \n",
    "            # Add values to dictionary\n",
    "            get_m[get_m_counter] = link.get('m')\n",
    "        \n",
    "            # Increment counter\n",
    "            get_m_counter += 1\n",
    "        \n",
    "        # Convert from string to dictionary\n",
    "        for key, value in get_m.items():\n",
    "            get_m[key] = json.loads(value)\n",
    "        \n",
    "        # Empty list to store image links\n",
    "        links = []\n",
    "    \n",
    "        # Loop through get_m dictionary and add image links to links list\n",
    "        for value in get_m.values():\n",
    "            links.append([value['murl'], gender])\n",
    "        \n",
    "        return links\n",
    "    else:\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of names to search\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popular names url\n",
    "name_url = 'https://www.ssa.gov/oact/babynames/decades/century.html'\n",
    "\n",
    "# Beautiful Soup object\n",
    "soup_name = all_links(name_url, 0)\n",
    "\n",
    "# Get td tag\n",
    "td_tag = soup_name.select('.t-stripe > tbody > tr > td')\n",
    "\n",
    "# Add name to search names\n",
    "names += [name.get_text().lower() for name in td_tag if name.get_text().isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for male names\n",
    "male_names = []\n",
    "\n",
    "# List for female names\n",
    "female_names = []\n",
    "\n",
    "for index, name in enumerate(names, 0):\n",
    "    if index % 2 == 0:\n",
    "        male_names.append([name, 0])\n",
    "    else:\n",
    "        female_names.append([name, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image links for each name\n",
    "def get_links(names):\n",
    "    \n",
    "    image_links_name = []\n",
    "    pre_image_links_count = 0\n",
    "    post_image_links_count = 0\n",
    "    \n",
    "    for index in range(len(names)):\n",
    "        \n",
    "        # Update url\n",
    "        url = 'https://www.bing.com/images/search?q=' + names[index][0] + '&qs=n&form=QBIR&qft=%20filterui%3Alicense-L2_L3_L4_L5_L6_L7%20filterui%3Aface-face&sp=-1&pq=' + names[index][0] + '&sc=8-9&sk=&cvid=8C2D8EC9A35A4948980A61669549C180'    \n",
    "    \n",
    "        # Number of image links before\n",
    "        pre_image_links_count = len(image_links_name)\n",
    "    \n",
    "        # Add image links\n",
    "        image_links_name += all_links(url, 1, names[index][1])\n",
    "        \n",
    "        # Number of image links after\n",
    "        post_image_links_count = len(image_links_name)\n",
    "        \n",
    "        # Number of image links associated with name\n",
    "        print('{} resulted in {} image links'.format(names[index][0], (post_image_links_count - pre_image_links_count)))\n",
    "        \n",
    "    return image_links_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for image links\n",
    "image_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "james resulted in 175 image links\n",
      "john resulted in 175 image links\n",
      "robert resulted in 175 image links\n",
      "michael resulted in 175 image links\n",
      "william resulted in 175 image links\n",
      "david resulted in 175 image links\n",
      "richard resulted in 210 image links\n",
      "joseph resulted in 175 image links\n",
      "thomas resulted in 175 image links\n",
      "charles resulted in 175 image links\n",
      "christopher resulted in 175 image links\n",
      "daniel resulted in 175 image links\n",
      "matthew resulted in 175 image links\n",
      "anthony resulted in 175 image links\n",
      "donald resulted in 175 image links\n",
      "mark resulted in 210 image links\n",
      "paul resulted in 175 image links\n",
      "steven resulted in 175 image links\n",
      "andrew resulted in 175 image links\n",
      "kenneth resulted in 175 image links\n",
      "joshua resulted in 210 image links\n",
      "george resulted in 175 image links\n",
      "kevin resulted in 175 image links\n",
      "brian resulted in 175 image links\n",
      "edward resulted in 175 image links\n",
      "ronald resulted in 175 image links\n",
      "timothy resulted in 175 image links\n",
      "jason resulted in 175 image links\n",
      "jeffrey resulted in 175 image links\n",
      "ryan resulted in 175 image links\n",
      "jacob resulted in 175 image links\n",
      "gary resulted in 175 image links\n",
      "nicholas resulted in 175 image links\n",
      "eric resulted in 175 image links\n",
      "stephen resulted in 210 image links\n",
      "jonathan resulted in 175 image links\n",
      "larry resulted in 175 image links\n",
      "justin resulted in 175 image links\n",
      "scott resulted in 175 image links\n",
      "brandon resulted in 175 image links\n",
      "frank resulted in 175 image links\n",
      "benjamin resulted in 175 image links\n",
      "gregory resulted in 175 image links\n",
      "samuel resulted in 175 image links\n",
      "raymond resulted in 175 image links\n",
      "patrick resulted in 175 image links\n",
      "alexander resulted in 175 image links\n",
      "jack resulted in 175 image links\n",
      "dennis resulted in 175 image links\n",
      "jerry resulted in 175 image links\n",
      "tyler resulted in 175 image links\n",
      "aaron resulted in 210 image links\n",
      "jose resulted in 210 image links\n",
      "henry resulted in 175 image links\n",
      "douglas resulted in 175 image links\n",
      "adam resulted in 175 image links\n",
      "peter resulted in 175 image links\n",
      "nathan resulted in 175 image links\n",
      "zachary resulted in 175 image links\n",
      "walter resulted in 175 image links\n",
      "kyle resulted in 210 image links\n",
      "harold resulted in 175 image links\n",
      "carl resulted in 491 image links\n",
      "jeremy resulted in 175 image links\n",
      "keith resulted in 175 image links\n",
      "roger resulted in 210 image links\n",
      "gerald resulted in 175 image links\n",
      "ethan resulted in 175 image links\n",
      "arthur resulted in 175 image links\n",
      "terry resulted in 175 image links\n",
      "christian resulted in 175 image links\n",
      "sean resulted in 175 image links\n",
      "lawrence resulted in 175 image links\n",
      "austin resulted in 175 image links\n",
      "joe resulted in 175 image links\n",
      "noah resulted in 210 image links\n",
      "jesse resulted in 175 image links\n",
      "albert resulted in 175 image links\n",
      "bryan resulted in 210 image links\n",
      "billy resulted in 175 image links\n",
      "bruce resulted in 175 image links\n",
      "willie resulted in 210 image links\n",
      "jordan resulted in 175 image links\n",
      "dylan resulted in 210 image links\n",
      "alan resulted in 175 image links\n",
      "ralph resulted in 175 image links\n",
      "gabriel resulted in 175 image links\n",
      "roy resulted in 175 image links\n",
      "juan resulted in 175 image links\n",
      "wayne resulted in 175 image links\n",
      "eugene resulted in 175 image links\n",
      "logan resulted in 175 image links\n",
      "randy resulted in 210 image links\n",
      "louis resulted in 175 image links\n",
      "russell resulted in 210 image links\n",
      "vincent resulted in 175 image links\n",
      "philip resulted in 175 image links\n",
      "bobby resulted in 210 image links\n",
      "johnny resulted in 175 image links\n",
      "bradley resulted in 175 image links\n"
     ]
    }
   ],
   "source": [
    "# Add male name image links\n",
    "image_links += get_links(male_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mary resulted in 175 image links\n",
      "patricia resulted in 175 image links\n",
      "jennifer resulted in 175 image links\n",
      "linda resulted in 175 image links\n",
      "elizabeth resulted in 175 image links\n",
      "barbara resulted in 175 image links\n",
      "susan resulted in 175 image links\n",
      "jessica resulted in 175 image links\n",
      "sarah resulted in 175 image links\n",
      "karen resulted in 175 image links\n",
      "nancy resulted in 175 image links\n",
      "margaret resulted in 175 image links\n",
      "lisa resulted in 175 image links\n",
      "betty resulted in 175 image links\n",
      "dorothy resulted in 175 image links\n",
      "sandra resulted in 613 image links\n",
      "ashley resulted in 175 image links\n",
      "kimberly resulted in 175 image links\n",
      "donna resulted in 175 image links\n",
      "emily resulted in 175 image links\n",
      "michelle resulted in 175 image links\n",
      "carol resulted in 175 image links\n",
      "amanda resulted in 175 image links\n",
      "melissa resulted in 175 image links\n",
      "deborah resulted in 175 image links\n",
      "stephanie resulted in 175 image links\n",
      "rebecca resulted in 175 image links\n",
      "laura resulted in 175 image links\n",
      "sharon resulted in 175 image links\n",
      "cynthia resulted in 175 image links\n",
      "kathleen resulted in 175 image links\n",
      "helen resulted in 175 image links\n",
      "amy resulted in 175 image links\n",
      "shirley resulted in 175 image links\n",
      "angela resulted in 175 image links\n",
      "anna resulted in 175 image links\n",
      "brenda resulted in 175 image links\n",
      "pamela resulted in 175 image links\n",
      "nicole resulted in 175 image links\n",
      "ruth resulted in 175 image links\n",
      "katherine resulted in 175 image links\n",
      "samantha resulted in 175 image links\n",
      "christine resulted in 175 image links\n",
      "emma resulted in 175 image links\n",
      "catherine resulted in 175 image links\n",
      "debra resulted in 175 image links\n",
      "virginia resulted in 175 image links\n",
      "rachel resulted in 175 image links\n",
      "carolyn resulted in 175 image links\n",
      "janet resulted in 175 image links\n",
      "maria resulted in 175 image links\n",
      "heather resulted in 175 image links\n",
      "diane resulted in 175 image links\n",
      "julie resulted in 175 image links\n",
      "joyce resulted in 175 image links\n",
      "victoria resulted in 175 image links\n",
      "kelly resulted in 6 image links\n",
      "christina resulted in 175 image links\n",
      "joan resulted in 175 image links\n",
      "evelyn resulted in 175 image links\n",
      "lauren resulted in 175 image links\n",
      "judith resulted in 175 image links\n",
      "olivia resulted in 210 image links\n",
      "frances resulted in 175 image links\n",
      "martha resulted in 175 image links\n",
      "cheryl resulted in 175 image links\n",
      "megan resulted in 175 image links\n",
      "andrea resulted in 175 image links\n",
      "hannah resulted in 175 image links\n",
      "jacqueline resulted in 175 image links\n",
      "ann resulted in 175 image links\n",
      "jean resulted in 175 image links\n",
      "alice resulted in 175 image links\n",
      "kathryn resulted in 175 image links\n",
      "gloria resulted in 175 image links\n",
      "teresa resulted in 175 image links\n",
      "doris resulted in 175 image links\n",
      "sara resulted in 175 image links\n",
      "janice resulted in 175 image links\n",
      "julia resulted in 175 image links\n",
      "marie resulted in 175 image links\n",
      "madison resulted in 175 image links\n",
      "grace resulted in 175 image links\n",
      "judy resulted in 175 image links\n",
      "theresa resulted in 175 image links\n",
      "beverly resulted in 175 image links\n",
      "denise resulted in 175 image links\n",
      "marilyn resulted in 210 image links\n",
      "amber resulted in 175 image links\n",
      "danielle resulted in 175 image links\n",
      "abigail resulted in 210 image links\n",
      "brittany resulted in 175 image links\n",
      "rose resulted in 175 image links\n",
      "diana resulted in 175 image links\n",
      "natalie resulted in 175 image links\n",
      "sophia resulted in 175 image links\n",
      "alexis resulted in 175 image links\n",
      "lori resulted in 175 image links\n",
      "kayla resulted in 175 image links\n",
      "jane resulted in 175 image links\n"
     ]
    }
   ],
   "source": [
    "# Add female name image links\n",
    "image_links += get_links(female_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36215 image links\n"
     ]
    }
   ],
   "source": [
    "print('There are {} image links'.format(len(image_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shuffle image links list\n",
    "random.shuffle(image_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 88 files removed due to a Request / Response Exception\n"
     ]
    }
   ],
   "source": [
    "# Header to accompany get request\n",
    "headers = {'user-agent': 'scraper - school project (bengriffith@outlook.com)'}\n",
    "\n",
    "# Counter to be used in image file name\n",
    "image_counter = 0\n",
    "\n",
    "# Counter for exceptions occurred\n",
    "exception_counter = 0\n",
    "\n",
    "# Write images to disk\n",
    "for image_link in image_links:\n",
    "    with open('images/face_' + str(image_counter) + '_' + str(image_link[1]) + '.png', 'wb') as f:\n",
    "        try:\n",
    "            response = requests.get(image_link[0], headers=headers)\n",
    "            \n",
    "            if response.ok:\n",
    "                f.write(response.content)\n",
    "        except:\n",
    "            exception_counter += 1\n",
    "            continue\n",
    "    image_counter += 1\n",
    "    \n",
    "print('There were {} files removed due to a Request / Response Exception'.format(exception_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = 'images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 466 files removed due to AttributeError\n"
     ]
    }
   ],
   "source": [
    "files_removed = 0\n",
    "\n",
    "for filename in os.listdir(images_directory)[1:]:\n",
    "    try:\n",
    "        image = cv2.imread(images_directory + filename)\n",
    "        (h, w) = image.shape[:2]\n",
    "    except AttributeError:\n",
    "        files_removed += 1\n",
    "        os.remove(images_directory + filename)\n",
    "        \n",
    "print('There were {} files removed due to AttributeError'.format(files_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxt = 'supporting/deploy.prototxt'\n",
    "model = 'supporting/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store detected face\n",
    "data = []\n",
    "\n",
    "# List to store labels\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35705"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(images_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_on_disk = os.listdir(images_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through five\n",
    "\n",
    "# Add five to list\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files_skipped = 0\n",
    "\n",
    "for i in range(1, len(os.listdir(images_directory))):\n",
    "    \n",
    "    # Image path\n",
    "    image_path = images_directory + images_on_disk[i]\n",
    "    \n",
    "    # Load model\n",
    "    #net = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    labels.append(images_on_disk[i][images_on_disk[i].find('.') - 1])\n",
    "    #(h, w) = image.shape[:2]\n",
    "    \n",
    "    # Blob for the image\n",
    "    #blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300))\n",
    "    \n",
    "    # Pass blob through network and retrieve detections / predictions\n",
    "    #net.setInput(blob)\n",
    "    #detections = net.forward()\n",
    "    \n",
    "    #for i in range(detections.shape[2]):\n",
    "    \n",
    "        # Get confidence associated with the prediction\n",
    "        #confidence = detections[0, 0, i, 2]\n",
    "    \n",
    "        \n",
    "        #if confidence > 0.3:\n",
    "        \n",
    "        # Coordinates for the bounding box\n",
    "        #box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        #(startX, startY, endX, endY) = box.astype('int')\n",
    "        \n",
    "        # Coordinates for face\n",
    "        #face = image[startY:endY, startX:endX]\n",
    "        #try:\n",
    "        #    face = cv2.resize(face, (96, 96))\n",
    "        #except:\n",
    "        #    files_skipped += 1\n",
    "        #    continue\n",
    "        \n",
    "        #face = img_to_array(face)\n",
    "        \n",
    "        # Populate data and labels lists\n",
    "        #data.append(face)\n",
    "        #labels.append(images_on_disk[i][images_on_disk[i].find('.') - 1])\n",
    "        #else:\n",
    "        #    continue\n",
    "        \n",
    "#print('There were {} files skipped due to Resize Error'.format(files_skipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays\n",
    "data = np.array(data, dtype='float') / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    input_shape = (height, width, depth)\n",
    "    channel_dimension = -1\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=channel_dimension))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=channel_dimension))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=channel_dimension))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=channel_dimension))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=channel_dimension))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "image_dimensions = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = ImageDataGenerator(rotation_range=25, vertical_flip=True, horizontal_flip=True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model = model(image_dimensions[1], image_dimensions[0], image_dimensions[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26778 samples, validate on 8926 samples\n",
      "Epoch 1/20\n",
      "26778/26778 [==============================] - 1755s 66ms/step - loss: 8.0855 - accuracy: 0.4928 - val_loss: 8.0444 - val_accuracy: 0.4954\n",
      "Epoch 2/20\n",
      " 7008/26778 [======>.......................] - ETA: 20:18 - loss: 8.1577 - accuracy: 0.4883"
     ]
    }
   ],
   "source": [
    "H = gender_model.fit(X_train, y_train, batch_size=batch_size, \n",
    "                     validation_data=(X_test, y_test), \n",
    "                     epochs=epochs, \n",
    "                     verbose=1)\n",
    "score = gender_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network to recognize whether male or female\n",
    "\n",
    "# If female then choose female GAN\n",
    "\n",
    "# If male then choose male GAN\n",
    "\n",
    "# Will need to create / train female and male GANs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
